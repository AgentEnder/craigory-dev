/*! /home/runner/work/craigory-dev/craigory-dev/libs/presentations/src/presentation-data/devup-2023-benchmarking/slides.md?raw [vike:pluginModuleBanner] */
const slides = 'layout: true\nname: base\n\nbackground-size: contain\n\n<div style="position: absolute; left: 20px; right: auto;" class="remark-slide-number">\n    <span style="display: block">DevUp 2023</span>\n    \n    August 28, 2023\n</div>\n\n---\n\ntemplate: base\n\n# Benchmarking like a Scientist:\n\n### Communicating Code\'s Performance.\n\n### Craigory Coppola\n\n<p>Slides: https://craigory.dev/presentations/view/devup-2023-benchmarking</p>\n\n---\n\n# Introduction\n\n- Previously full stack developer, with a focus on Angular and .NET\n- Core team member working on Nx, a task orchestrator used to speed up workspaces via caching and parallelization.\n\n<div style="display: flex">\n    <img style="max-width: 30%" src="/assets/that-conf-wi-2023/nrwl-logo.svg" alt="Nrwl Logo"></img>\n    <img style="max-width: 30%" src="/assets/that-conf-wi-2023/nx-logo.svg" alt="Nx Logo"></img>\n</div>\n\n???\n\n- Nx Helps speed up development by caching and parallelizing tasks.\n  - Also graph visualization.\n  - Initially heavy focus on monorepo tooling, but has since expanded to support single repos as well.\n  - Clients with 1k+ projects in a single workspace, so we have to be very careful about performance.\n  - Value is based on speeding up development, so we have to be very careful about any overhead our tool adds.\n  - Frontend tooling is getting faster overall, so any overhead we add is a larger slice of the pie now.\n\n---\n\n# Agenda\n\n1. What is Benchmarking?\n1. Why Benchmarking?\n1. The Scientific Method\n2. Implementation\n3. Transparency and Honesty\n\n---\n\n# What is Benchmarking?\n\n- A benchmark is a measurement, or a point of reference.\n- Benchmarking is the process of collecting and analyzing benchmarks.\n\n---\n\n# What is Benchmarking?\n\n- Benchmarks are used in almost every industry.\n- They can track:\n  - Performance\n  - Quality\n  - Compliance\n\n---\n\n# What is Benchmarking?\n\n- In programming, these may look like:\n  - Performance: how fast does your program run?\n  - Quality: how many bugs does your program have?\n  - Compliance: does your program meet its requirements?\n\n\n---\n\n# Why Benchmarking?\n\nThere are 3 main categories of reasons to benchmark:\n\n- Checking things for yourself\n- Checking things for your peers\n- Checking things against your competitors\n\n???\n\n- Checking things for yourself\n  - Is this new library faster than the old one?\n  - Is this new language faster than the old one?\n  - Is this new algorithm faster than the old one?\n- Checking things for your peers\n  - Is this change worth the effort?\n  - Should we invest in this new technology?\n  - Has our performance slipped at all?\n  - Do we need to invest in performance?\n- Checking things against your competitors\n  - Does our product perform better than our competitors?\n  - Do the pages feel snappier than our competitors?\n  - Do customers spend less time waiting for our product than our competitors?\n\n---\n\n# Checking things for yourself\n\n- Is this new library faster than the old one?\n  - Should I spend time to learn it?\n  - Is this PR ready to be reviewed?\n\n---\n\n# Checking things for your peers\n\n- Is this change worth it?\n  - Do any performance benefits outweigh the maintenance cost of the change?\n- Has our performance slipped at all?\n- Should we invest in this new technology?\n- Do we need to invest in performance?\n  - Is this an area we need to focus on?\n\n---\n\n# Checking things against your competitors\n\n- Are there competitors that are faster than us?\n  - What are they doing differently?\n- Even if we are faster, are we fast enough?\n  - Do our customers notice the difference?\n  - Do our customers care about the difference?\n\n---\n\n# The Benchmarks You Already Have\n\n- How long does it take to run your E2E tests?\n- How long does it take to run your unit tests?\n- How large is your bundle size?\n\n???\n\n- These are all benchmarks that you can use to compare your codebase over time.\n- E2E tests are a great way to measure the performance of your application, and can be used to compare the performance of your application over time. If your E2E tests are taking longer to run, you can use this as a signal that your application is getting slower.\n- Does any 1 unit test take a long time to run? This can be a signal that the code under test is slow, or that the test itself is slow. Look at the test, and evaluate if the slow unit test reflects slow runtime code.\n- Has your bundle increased in size? The bundle size of your application is reflected in how long your users will have to wait to download your application. If your bundle size is increasing, you can use this as a signal that your application may feel slower. \n\n---\n\n# The Scientific Method\n\n- Used to support a given hypothesis\n- Provides a testable theory as a result\n- Easy to revisit and see if a given theory still holds.\n\n---\n\n.center[<img src="https://upload.wikimedia.org/wikipedia/commons/8/82/The_Scientific_Method.svg" style="max-width: 50%; margin: auto auto"></img>]\n\n---\n\n# The Benchmarks You Already Have, Revisited\n\n- Observation: Your E2E suite is taking longer to run\n- Research Topic Area: Which features are responsible for the slowdown?\n- Hypothesis: The slowdown is the result of recent changes.\n- Test Hypothesis: Check if the jump in E2E duration occurred after a specific commit.\n- Analyze Data: Look at the identified commit, and see if there are any obvious culprits.\n- Report Conclusions: Reach out to your teammates with the findings and see how to best proceed.\n\n???\n\n- Test Hypothesis: Does `git bisect` possibly help here? It could, but if the e2e suite takes more than a minute or two to run it could be prohibitively slow. Instead, I\'d look at CI provider logs over time.\n\n---\n\n# Writing new benchmarks\n\n- Keep things simple.\n- Determine what you want to measure. (time, memory, bundle size, etc.)\n- Measure the thing you want to measure.\n- Record results.\n\n---\n\n# Benchmarks are comparisons\n\n- Measure both states\n  - Original vs modified\n  - Tech A vs Tech B\n\n---\n\n# Case Study #1: Nx Large Monorepo\n\n- Benchmark\'s Nx\'s performance on a large monorepo.\n- We\'ve identified number of projects as a factor in overall performance.\n- Run\'s competitors tooling as well, to see how we stack up.\n\n---\n\n.center[<img src="https://upload.wikimedia.org/wikipedia/commons/8/82/The_Scientific_Method.svg" style="max-width: 50%; margin: auto auto"></img>]\n\n???\n\nExample:\n\n- Observation: Turborepo announces that it is faster than Nx.\n- Research Topic Area: Time added from tooling overhead. Easily measured by time to run a cached command.\n- Hypothesis: The stated difference is inaccurate.\n- Test Hypothesis: Run the same command in both tools and compare the results. Vercel\'s benchmarks aren\'t public, so we\'ll have to run our own.\n- Analyze Data: Compare the results of the two tools.\n- Report Conclusions: Publish the results of the benchmark, with source code and methodology.\n\n---\n\n# Case Study #1: Nx Large Monorepo\n\n## For your peers.\n\n<div style="display: grid; grid-template-columns: 1fr 1fr">\n<ul>\n    <li>https://github.com/vsavkin/large-monorepo</li>\n    <li>"E2E Benchmark"\n        <ul>\n            <li>\n                Measures the time to run a cached command in each tool, from the terminal.\n            </li>\n        </ul>\n    </li>\n</ul>\n<img style="max-width: 100%" src="/assets/that-conf-wi-2023/turbo-nx-perf.gif" alt="nx + turborepo benchmark comparison"></img>\n</div>\n\n???\n\n- Turborepo announces that it is faster than Nx.\n- We don\'t believe it, but don\'t want to just say that without any backing.\n\n---\n\n# Case Study #1: Nx Large Monorepo\n\n## Implementation\n\n```typescript\nlet start = new Date().getTime();\nfor (let i = 0; i < NUMBER_OF_RUNS; ++i) {\n  spawnSync(\'turbo\', [\'run\', \'build\', `--concurrency=10`]);\n}\nconst turboTime = new Date().getTime() - start;\nstart = new Date().getTime();\nfor (let i = 0; i < NUMBER_OF_RUNS; ++i) {\n  spawnSync(\'nx\', [\'run-many\', \'-t\', \'build\', \'--parallel\', 10]);\n}\nconst nxTime = new Date().getTime() - start;\nconst averageNxTime = nxTime / NUMBER_OF_RUNS;\nconst averageTurboTime = turboTime / NUMBER_OF_RUNS;\n```\n\n---\n\n# Case Study #2: `findMatchingProjects`\n\n- Internal function in Nx codebase\n- Called during several important CLI commands and graph construction\n- "Hot Path" for performance\n- Seems like it could be faster.\n\n---\n\n.center[<img src="https://upload.wikimedia.org/wikipedia/commons/8/82/The_Scientific_Method.svg" style="max-width: 50%; margin: auto auto"></img>]\n\n???\n\n- Observation: Flame chart reveals minimatch, a library we use to match glob patterns, is taking the majority of the time. For each project pattern we look at, minimatch compiles the glob pattern to a regular expression and evaluates it. This is expensive, and we can avoid it by caching the results.\n- Research Topic Area: Single unit - similar to a unit test\n- Hypothesis: Caching the results of minimatch will improve performance.\n- Test Hypothesis: Establish a baseline for the function, then add caching and compare the results. Ensure that the worst-case is comparable to the original function, and the best-case is faster.\n- Analyze Data: Record the benchmark result as an expected duration.\n- Report Conclusions: Write a unit test that compares the runtime of the function to the expected duration.\n\n---\n\n# Case Study #2: `findMatchingProjects`\n\n- Observation: Flame chart reveals minimatch, a library we use to match glob patterns, is taking the majority of the time. For each project pattern we look at, minimatch compiles the glob pattern to a regular expression and evaluates it. This is expensive, and we can avoid it by caching the results.\n\n--\n\n- Research Topic Area: Single unit - similar to a unit test\n\n--\n\n- Hypothesis: Caching the results of minimatch will improve performance.\n\n--\n\n- Test Hypothesis: Establish a baseline for the function, then add caching and compare the results. Ensure that the worst-case is comparable to the original function, and the best-case is faster.\n\n--\n\n- Analyze Data: Record the benchmark result as an expected duration.\n\n--\n\n- Report Conclusions: Write a unit test that compares the runtime of the function to the expected duration.\n\n---\n\n# Case Study #2: `findMatchingProjects`\n\n```typescript\nit(`should be faster than using minimatch directly multiple times (${pattern})`, () => {\n  const iterations = 100;\n  const cacheTime = time(() => getMatchingStringsWithCache(pattern, items), iterations);\n  const directTime = time(() => minimatch.match(items, pattern), iterations);\n  // Using minimatch directly takes at least twice as long than using the cache.\n  expect(directTime / cacheTime).toBeGreaterThan(2);\n});\n\nit(`should be comparable to using minimatch a single time (${pattern})`, () => {\n  const cacheTime = time(() => getMatchingStringsWithCache(pattern, items));\n  const directTime = time(() => minimatch.match(items, pattern));\n  // We are dealing with really small file sets here, with such a small\n  // difference it time, the system variablility can make this flaky for\n  // smaller values. If we are within 1ms, we are good.\n  expect(cacheTime).toBeLessThan(directTime + 1);\n});\n```\n\n---\n\n# Case Study #3: Runtime Benchmarks\n\n## For your users.\n\n- Support ticket is raised:\n  - "This page is slow when I access it"\n  - "This command takes a long time to run"\n  - "Should this be this slow?"\n- How can you help answer, when you can\'t reproduce the slowness?\n\n???\n\n- Users won\'t be able to provide you with a flame chart, or a unit-level benchmark result.\n- Need some form of instrumentation to help you understand what is happening in the wild.\n\n---\n\n.center[<img src="https://upload.wikimedia.org/wikipedia/commons/8/82/The_Scientific_Method.svg" style="max-width: 50%; margin: auto auto"></img>]\n\n---\n\n# Case Study #3: Runtime Benchmarks\n\n- Observation: User reports that a command is slow.\n- Research Topic Area: Entire system\n- Hypothesis: Instrumenting the command will help us understand what is happening, and help us reproduce the issue.\n- Test Hypothesis: Provide a flag that enables instrumentation, and let the user run the command.\n- Analyze Data: User provided logs show a slow section of code.\n- Report Conclusions: Issue is resolved, or we have enough information to reproduce the issue locally.\n\n???\n\n- We can\'t reproduce it locally, so we need to get more information.\n\n---\n\n# Case Study #3\n\n```typescript\nimport { PerformanceObserver } from \'perf_hooks\';\n\nif (process.env.NX_PERF_LOGGING === \'true\') {\n  const obs = new PerformanceObserver((list) => {\n    for (const entry of list.getEntries()) {\n      console.log(`Time for \'${entry.name}\'`, entry.duration);\n    }\n  });\n  obs.observe({ entryTypes: [\'measure\'] });\n}\n```\n\n> https://github.com/nrwl/nx/blob/master/packages/nx/src/utils/perf-logging.ts\n\n---\n\n# Transparency and Repeatability\n\n- Publish the results of your benchmarks\n- Make the source code available to stakeholders\n  - If comparing internally, this is your team\n  - If comparing to an external tool, and sharing the results publicly, this is the general public.\n\n---\n\n# Automated Benchmarks\n\n- Web vitals / Lighthouse\n- Unit Test Performance\n- Bundle Size restrictions\n\n???\n\n- Run lighthouse reports during CI, if the score drops below a certain threshold, fail the build.\n- Tune the timeouts for your test suites. By default most frameworks will fail if a test takes longer than a certain amount of time, but you can tune this to be more strict. If a few tests take longer change the timeout for them, not the entire suite.\n  - Jest + Mocha default: 5s\n  - MSTest / NUnit / XUnit have no default timeout. MSTest + NUnit can be specified, XUnit requires something custom.\n- Fail the build if the bundle size is too large. Angular has a built-in mechanism for this, but you can also use a tool like size-limit.\n\n---\n\n# Web Vitals and Lighthouse\n\n- Web Vitals: https://web.dev/vitals/\n  - Cypress Plugin: https://www.npmjs.com/package/cypress-web-vitals\n- Lighthouse CI:\n  - https://github.com/GoogleChrome/lighthouse-ci\n\n---\n\n# Unit Test Performance\n\n- Jest Timeouts example:\n\n```typescript\ndescribe(\'feature-tests\', () => {\n  it(\'a short test\', () => {\n    expect(true).toBe(true);\n  }, 100);\n\n  it(\'a long test\', () => {\n    expect(someLongOperation()).toBe(true);\n  }, 10000);\n});\n```\n\n---\n\n# Bundle Size Restrictions\n\n- Example from Angular configuration\n\n```json5\n{\n  budgets: [\n    {\n      type: \'initial\',\n      maximumWarning: \'2mb\',\n      maximumError: \'5mb\',\n    },\n  ],\n}\n```\n\n- Could also use a tool like `size-limit`: https://www.npmjs.com/package/size-limit\n\n---\n\n# Presenting Results\n\n- Consider your audience.\n  - Devs? Code alone may be fine\n  - Higher ups? Graphs and charts are likely better\n- Consider aggregation methods + appropriate statistics\n  - x% faster on average\n  - worst case is x% faster\n  - 95th percentile etc\n  \n---\n\n# Wrapping up:\n\n- You probably already have benchmarks\n- Make sure they are reproducible\n- Make sure they are transparent\n- Make sure they have purpose\n\n---\n\nbackground-image: url(/assets/devup-2023/sponsors.jpeg)\n\n---\n\n<div style="display: grid; grid-template-columns: 1fr 1fr; height: 100%">\n<div><h2>Questions?</h2></div>\n<div style="display: flex; flex-direction: column; align-items: center; align-self: center">\n  <img src="/assets/ProfilePic_cropped.png" style="max-width: 50%"/>\n  <h2>Contact + Links</h2>\n  <ul>\n    <li>Twitter (x?): @EnderAgent</li>\n    <li>https://www.linkedin.com/in/craigoryvcoppola/</li>\n    <li>https://github.com/agentender</li>\n    <li>https://craigory.dev</li>\n  </ul>\n</div> \n</div>\n';

export { slides as default };
